{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simple Neural Network in python \n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First Line of The Codes from Reference and \n",
    "# Assignment 2A, in th Last Assignment is in Concise Codes , \n",
    "#For Assignment 2B.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 0]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]]\n",
      "[[1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "#Step 0 : Read Input and Output\n",
    "eip_in = np.array([[1,0,1,0],[1,0,1,1],[0,1,0,1]])\n",
    "eip = np.array([[1],[1],[0]])\n",
    "print(eip_in)\n",
    "print(eip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights  [[ 0.42  0.88  0.55]\n",
      " [ 0.1   0.73  0.68]\n",
      " [ 0.6   0.18  0.47]\n",
      " [ 0.92  0.11  0.52]]\n",
      "-----------------\n",
      "Bias  [ 0.46  0.72  0.08]\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Initialize weights and biases with random values\n",
    "##(There are methods to initialize weights and biases but for now initialize with random values)\n",
    "\n",
    "mlblr = np.array([[0.42,0.88,0.55],[0.1,0.73,0.68],[0.6,0.18,0.47],[0.92,0.11,0.52]])\n",
    "print(\"Weights \",mlblr)\n",
    "print(\"-----------------\")\n",
    "bh = np.array([0.46,0.72,0.08])\n",
    "print(\"Bias \",bh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Inputs\n",
      " [[ 1.48  1.78  1.1 ]\n",
      " [ 2.4   1.89  1.62]\n",
      " [ 1.48  1.56  1.28]]\n"
     ]
    }
   ],
   "source": [
    "#Step 2 Calculate hidden layer input:\n",
    "mlblr_in = np.dot(eip_in,mlblr) + bh\n",
    "print(\"Hidden Layer Inputs\\n\",mlblr_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Activations\n",
      " [[ 0.81457258  0.85569687  0.75026011]\n",
      " [ 0.9168273   0.86875553  0.83479513]\n",
      " [ 0.81457258  0.82635335  0.78244978]]\n"
     ]
    }
   ],
   "source": [
    "#Step 3: Perform non-linear transformation on hidden linear input\n",
    "#hiddenlayer_activations = sigmoid(hidden_layer_input)\n",
    "def sigmoid(x, derivative=False):\n",
    " return x*(1-x) if derivative else 1/(1+np.exp(-x))\n",
    "\n",
    "mlblr_activations = sigmoid(mlblr_in)\n",
    "print(\"Hidden Layer Activations\\n\",mlblr_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output :\n",
      " [[ 0.78932406]\n",
      " [ 0.79806432]\n",
      " [ 0.78933532]]\n"
     ]
    }
   ],
   "source": [
    "#Step 4: Perform linear and non-linear transformation of hidden layer activation at output layer\n",
    "#output_layer_input = matrix_dot_product (hiddenlayer_activations * wout ) + bout\n",
    "#output = sigmoid(output_layer_input)\n",
    "wout = np.array([[0.3],[0.25],[0.23]])\n",
    "bout = np.array([0.69])\n",
    "\n",
    "mlblr_out = np.dot(mlblr_activations,wout) + bout\n",
    "eip_out = sigmoid(mlblr_out)\n",
    "print(\"Output :\\n\",eip_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.21067594]\n",
      " [ 0.20193568]\n",
      " [-0.78933532]]\n"
     ]
    }
   ],
   "source": [
    "#Step 5: Calculate gradient of Error(E) at output layer\n",
    "#Error = y - output\n",
    "\n",
    "E = eip - eip_out\n",
    "print(E)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope_output_layer \n",
      " [[ 0.16629159]\n",
      " [ 0.16115766]\n",
      " [ 0.16628507]]\n",
      "\n",
      "Slope_hidden_layer \n",
      " [[ 0.15104409  0.12347974  0.18736988]\n",
      " [ 0.076255    0.11401936  0.13791222]\n",
      " [ 0.15104409  0.14349349  0.17022212]]\n"
     ]
    }
   ],
   "source": [
    "#Step 6: Compute slope at output and hidden layer\n",
    "#`Slope_output_layer= derivatives_sigmoid(output)\n",
    "\n",
    "#Slope_hidden_layer = derivatives_sigmoid(hidden_layer_activations)`\n",
    "#Sigmoid Function\n",
    "def derivative_sigmoid(x):\n",
    "    return x * (1 - x)\n",
    "#`Slope_output_layer = derivatives_sigmoid(output)\n",
    "Slope_eip_out = derivative_sigmoid(eip_out)\n",
    "print(\"Slope_output_layer \\n\",Slope_eip_out)\n",
    "\n",
    "#Slope_hidden_layer = derivatives_sigmoid(hidden_layer_activations)`\n",
    "Slope_mlblr =  derivative_sigmoid(mlblr_activations)\n",
    "print(\"\\nSlope_hidden_layer \\n\",Slope_mlblr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta at output layer \n",
      " [[ 0.03503364]\n",
      " [ 0.03254348]\n",
      " [-0.13125468]]\n"
     ]
    }
   ],
   "source": [
    "#Step 7: Compute delta at output layer\n",
    "# d_output = E * slope_output_layer*lr\n",
    "lr = 0.01 # learning Rate\n",
    "d_eip_out = E * Slope_eip_out\n",
    "print(\"delta at output layer \\n\", d_eip_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error_at_hidden_layer \n",
      " [[ 0.01051009  0.00875841  0.00805774]\n",
      " [ 0.00976304  0.00813587  0.007485  ]\n",
      " [-0.0393764  -0.03281367 -0.03018858]]\n"
     ]
    }
   ],
   "source": [
    "#Step 8: Calculate Error at hidden layer\n",
    "#Error_at_hidden_layer = matrix_dot_product(d_output, wout.Transpose)\n",
    "E_mlblr_in = np.dot(d_eip_out, wout.T)\n",
    "print(\"Error_at_hidden_layer \\n\",E_mlblr_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " delta at hidden layer \n",
      " [[ 0.00158749  0.00108149  0.00150978]\n",
      " [ 0.00074448  0.00092765  0.00103227]\n",
      " [-0.00594757 -0.00470855 -0.00513876]]\n"
     ]
    }
   ],
   "source": [
    "#Step 9: Compute delta at hidden layer\n",
    "#d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer\n",
    "\n",
    "d_mlblr_in = E_mlblr_in * Slope_mlblr\n",
    "print(\" delta at hidden layer \\n\",d_mlblr_in)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Weights at the Output Layer \n",
      " [[ 0.29951458]\n",
      " [ 0.24949788]\n",
      " [ 0.22950751]] \n",
      "---------------\n",
      "\n",
      "New Weights in Hidden Layer \n",
      " [[ 0.42002332  0.88002009  0.55002542]\n",
      " [ 0.09994052  0.72995291  0.67994861]\n",
      " [ 0.60002332  0.18002009  0.47002542]\n",
      " [ 0.91994797  0.10996219  0.51995894]]\n"
     ]
    }
   ],
   "source": [
    "#Step 10: Update weight at both output and hidden layer\n",
    "#wout = wout + matrix_dot_product (hiddenlayer_activations.Transpose, d_output) * learning_rate\n",
    "wout = wout + np.dot(mlblr_activations.T,d_eip_out)*lr\n",
    "print(\"New Weights at the Output Layer \\n\",wout,\"\\n---------------\\n\")\n",
    "\n",
    "#wh = wh+ matrix_dot_product (X.Transpose,d_hiddenlayer) * learning_rate\n",
    "mlblr = mlblr + np.dot(eip_in.T,d_mlblr_in) *lr\n",
    "print(\"New Weights in Hidden Layer \\n\",mlblr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Bias at the Output Layer \n",
      " [[ 0.68936322]] \n",
      "---------------\n",
      "\n",
      "New Bias in Hidden Layer \n",
      " [ 0.45996384  0.71997301  0.07997403]\n"
     ]
    }
   ],
   "source": [
    "#Step 11: Update biases at both output and hidden layer\n",
    "#bout = bout + sum(d_output, axis=0)*learning_rate\n",
    "bout = bout + np.sum(d_eip_out,axis=0, keepdims=True)*lr\n",
    "print(\"New Bias at the Output Layer \\n\",bout,\"\\n---------------\\n\")\n",
    "#bh = bh + sum(d_hiddenlayer, axis=0) * learning_rate\n",
    "bh = bh + np.sum(d_mlblr_in, axis=0)*lr\n",
    "print(\"New Bias in Hidden Layer \\n\",bh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X \n",
      " [[1 0 1 0]\n",
      " [1 0 1 1]\n",
      " [0 1 0 1]]\n",
      "y \n",
      " [[1]\n",
      " [1]\n",
      " [0]]\n",
      "Random Weights and Bias numbers are choosen \n",
      "\n",
      "wh1 \n",
      " [[ 0.98843715  0.73626613  0.66268227]\n",
      " [ 0.86914818  0.97597204  0.7479577 ]\n",
      " [ 0.61006716  0.86879147  0.71780495]\n",
      " [ 0.2937741   0.6825182   0.0073115 ]]\n",
      "bh1 \n",
      " [[ 0.79383757  0.29845415  0.12919281]]\n",
      "Wout1 \n",
      " [[ 0.91897081]\n",
      " [ 0.99616049]\n",
      " [ 0.16463027]]\n",
      "Bout1 \n",
      " [[ 0.59926697]]\n",
      "\n",
      " Epoch No :  1\n",
      "hidden layer input \n",
      " [[ 2.39234188  1.90351176  1.50968003]\n",
      " [ 2.68611597  2.58602995  1.51699153]\n",
      " [ 1.95675985  1.95694439  0.88446201]]\n",
      "hiddenlayer activations \n",
      " [[ 0.91624147  0.87028847  0.81901378]\n",
      " [ 0.93620239  0.92995706  0.82009504]\n",
      " [ 0.87618187  0.87620188  0.707746  ]]\n",
      " output layer input \n",
      " [[ 2.44304759]\n",
      " [ 2.52100859]\n",
      " [ 2.39380665]]\n",
      "output \n",
      " [[ 0.92005155]\n",
      " [ 0.92560154]\n",
      " [ 0.91635381]]\n",
      "Error or Loss \n",
      " [[ 0.07994845]\n",
      " [ 0.07439846]\n",
      " [-0.91635381]]\n",
      "slope output layer \n",
      " [[ 0.0735567 ]\n",
      " [ 0.06886333]\n",
      " [ 0.07664951]]\n",
      "slope hidden layer \n",
      " [[ 0.07674304  0.11288645  0.14823021]\n",
      " [ 0.05972747  0.06513693  0.14753917]\n",
      " [ 0.1084872   0.10847214  0.2068416 ]]\n",
      "delta at output \n",
      " [[ 0.00588074]\n",
      " [ 0.00512333]\n",
      " [-0.07023807]]\n",
      "Error at hidden layer \n",
      " [[ 0.00540423  0.00585817  0.00096815]\n",
      " [ 0.00470819  0.00510365  0.00084345]\n",
      " [-0.06454673 -0.06996839 -0.01156331]]\n",
      "delta at hidden layer \n",
      " [[ 0.91897081]\n",
      " [ 0.99616049]\n",
      " [ 0.16463027]]\n",
      "New Weights at Output Layer\n",
      " [[ 0.91383515]\n",
      " [ 0.99099445]\n",
      " [ 0.16056101]]\n",
      " New Bias at Output Layer \n",
      " [[ 0.59334357]]\n",
      "New Weights at Hidden Layer\n",
      " [[ 0.98850674  0.73636551  0.66270906]\n",
      " [ 0.86844793  0.97521308  0.74771852]\n",
      " [ 0.61013676  0.86889085  0.71783174]\n",
      " [ 0.29310197  0.68179248  0.00708477]]\n",
      " New Bias at Hidden Layer \n",
      " [[ 0.79320692  0.29779456  0.12898043]]\n",
      "\n",
      " Epoch No :  2\n",
      "hidden layer input \n",
      " [[ 2.39185041  1.90305092  1.50952123]\n",
      " [ 2.68495238  2.5848434   1.516606  ]\n",
      " [ 1.95475681  1.95480012  0.88378372]]\n",
      "hiddenlayer activations \n",
      " [[ 0.91620374  0.87023644  0.81899024]\n",
      " [ 0.93613286  0.92987973  0.82003815]\n",
      " [ 0.8759644   0.8759691   0.70760569]]\n",
      " output layer input \n",
      " [[ 2.42450014]\n",
      " [ 2.50198649]\n",
      " [ 2.37552503]]\n",
      "output \n",
      " [[ 0.91867658]\n",
      " [ 0.92428096]\n",
      " [ 0.91494182]]\n",
      "Error or Loss \n",
      " [[ 0.08132342]\n",
      " [ 0.07571904]\n",
      " [-0.91494182]]\n",
      "slope output layer \n",
      " [[ 0.07470992]\n",
      " [ 0.06998566]\n",
      " [ 0.07782328]]\n",
      "slope hidden layer \n",
      " [[ 0.07677445  0.11292498  0.14824523]\n",
      " [ 0.05978813  0.06520341  0.14757558]\n",
      " [ 0.10865077  0.10864723  0.20689988]]\n",
      "delta at output \n",
      " [[ 0.00607567]\n",
      " [ 0.00529925]\n",
      " [-0.07120378]]\n",
      "Error at hidden layer \n",
      " [[ 0.00555216  0.00602095  0.00097552]\n",
      " [ 0.00484264  0.00525152  0.00085085]\n",
      " [-0.06506851 -0.07056255 -0.01143255]]\n",
      "delta at hidden layer \n",
      " [[ 0.91383515]\n",
      " [ 0.99099445]\n",
      " [ 0.16056101]]\n",
      "New Weights at Output Layer\n",
      " [[ 0.90865068]\n",
      " [ 0.98577872]\n",
      " [ 0.15645474]]\n",
      " New Bias at Output Layer \n",
      " [[ 0.58736068]]\n",
      "New Weights at Hidden Layer\n",
      " [[ 0.98857832  0.73646774  0.66273608]\n",
      " [ 0.86774096  0.97444644  0.74748198]\n",
      " [ 0.61020834  0.86899308  0.71785876]\n",
      " [ 0.29242395  0.68106008  0.00686078]]\n",
      " New Bias at Hidden Layer \n",
      " [[ 0.79257152  0.29713015  0.12877091]]\n",
      "\n",
      " Epoch No :  3\n",
      "hidden layer input \n",
      " [[ 2.39135818  1.90259098  1.50936575]\n",
      " [ 2.68378212  2.58365105  1.51622653]\n",
      " [ 1.95273642  1.95263667  0.88311367]]\n",
      "hiddenlayer activations \n",
      " [[ 0.91616594  0.87018449  0.81896719]\n",
      " [ 0.93606285  0.92980195  0.81998215]\n",
      " [ 0.87574471  0.87573386  0.70746703]]\n",
      " output layer input \n",
      " [[ 2.40577614]\n",
      " [ 2.4827839 ]\n",
      " [ 2.35707309]]\n",
      "output \n",
      " [[ 0.9172667 ]\n",
      " [ 0.92292606]\n",
      " [ 0.91349479]]\n",
      "Error or Loss \n",
      " [[ 0.0827333 ]\n",
      " [ 0.07707394]\n",
      " [-0.91349479]]\n",
      "slope output layer \n",
      " [[ 0.0758885 ]\n",
      " [ 0.07113355]\n",
      " [ 0.07902205]]\n",
      "slope hidden layer \n",
      " [[ 0.07680591  0.11296344  0.14825993]\n",
      " [ 0.05984919  0.06527028  0.14761143]\n",
      " [ 0.10881591  0.10882407  0.20695743]]\n",
      "delta at output \n",
      " [[ 0.00627851]\n",
      " [ 0.00548254]\n",
      " [-0.07218624]]\n",
      "Error at hidden layer \n",
      " [[ 0.00570497  0.00618922  0.0009823 ]\n",
      " [ 0.00498172  0.00540457  0.00085777]\n",
      " [-0.06559207 -0.07115965 -0.01129388]]\n",
      "delta at hidden layer \n",
      " [[ 0.90865068]\n",
      " [ 0.98577872]\n",
      " [ 0.15645474]]\n",
      "New Weights at Output Layer\n",
      " [[ 0.90341743]\n",
      " [ 0.98051324]\n",
      " [ 0.15231155]]\n",
      " New Bias at Output Layer \n",
      " [[ 0.58131817]]\n",
      "New Weights at Hidden Layer\n",
      " [[ 0.98865195  0.73657293  0.66276331]\n",
      " [ 0.86702721  0.97367205  0.74724824]\n",
      " [ 0.61028197  0.86909827  0.71788598]\n",
      " [ 0.29174001  0.68032097  0.00663971]]\n",
      " New Bias at Hidden Layer \n",
      " [[ 0.79193141  0.29646096  0.1285644 ]]\n",
      "\n",
      " Final Output \n",
      " [[ 0.9172667 ]\n",
      " [ 0.92292606]\n",
      " [ 0.91349479]]\n"
     ]
    }
   ],
   "source": [
    "### Assignment 2B : Python File in Concise Program..\n",
    "import numpy as np\n",
    "\n",
    "#Input array\n",
    "X = np.array([[1,0,1,0],[1,0,1,1],[0,1,0,1]])\n",
    "print(\"X \\n\",X)\n",
    "#Output\n",
    "y = np.array([[1],[1],[0]])\n",
    "print(\"y \\n\",y)\n",
    "#Sigmoid Function\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "#Derivative of Sigmoid Function\n",
    "def derivatives_sigmoid(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "#Variable initialization\n",
    "##----------------------###\n",
    "#Setting training iterations\n",
    "epochs = 3\n",
    "\n",
    "#Setting learning rate\n",
    "lr1 = 0.1 \n",
    "\n",
    "#number of features in data set\n",
    "inputlayer_neurons = X.shape[1]\n",
    "\n",
    "#number of hidden layers neurons\n",
    "hiddenlayer_neurons = 3 \n",
    "\n",
    "#number of neurons at output layer\n",
    "output_neurons = 1\n",
    "\n",
    "#weight and bias initialization\n",
    "print(\"Random Weights and Bias numbers are choosen \\n\")\n",
    "wh1 = np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))\n",
    "print(\"wh1 \\n\",wh1)\n",
    "bh1 = np.random.uniform(size=(1,hiddenlayer_neurons))\n",
    "print(\"bh1 \\n\",bh1)\n",
    "wout1 = np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
    "print(\"Wout1 \\n\",wout1)\n",
    "bout1 = np.random.uniform(size=(1,output_neurons))\n",
    "print(\"Bout1 \\n\",bout1)\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(\"\\n Epoch No : \",i+1)\n",
    "    #Forward Propogation\n",
    "    hidden_layer_input1 = np.dot(X,wh1)\n",
    "    \n",
    "    hidden_layer_input = hidden_layer_input1 + bh1\n",
    "    print(\"hidden layer input \\n\",hidden_layer_input)\n",
    "\n",
    "    hiddenlayer_activations = sigmoid(hidden_layer_input)\n",
    "    print(\"hiddenlayer activations \\n\",hiddenlayer_activations)\n",
    "\n",
    "    output_layer_input1 = np.dot(hiddenlayer_activations,wout1) \n",
    "        \n",
    "    output_layer_input = output_layer_input1+ bout1\n",
    "    print(\" output layer input \\n\",output_layer_input)\n",
    "    \n",
    "    output = sigmoid(output_layer_input)\n",
    "    print(\"output \\n\",output)\n",
    "    \n",
    "    #Backpropagation\n",
    "    E = y-output\n",
    "    print(\"Error or Loss \\n\",E)\n",
    "    \n",
    "    slope_output_layer = derivatives_sigmoid(output)\n",
    "    print(\"slope output layer \\n\",slope_output_layer)\n",
    "    \n",
    "    slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)\n",
    "    print(\"slope hidden layer \\n\",slope_hidden_layer)\n",
    "\n",
    "    d_output = E * slope_output_layer\n",
    "    print(\"delta at output \\n\", d_output)\n",
    "\n",
    "    Error_at_hidden_layer = d_output.dot(wout1.T)\n",
    "    print(\"Error at hidden layer \\n\",Error_at_hidden_layer)\n",
    "\n",
    "    d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer\n",
    "    print(\"delta at hidden layer \\n\",wout1)\n",
    "\n",
    "    wout1 += hiddenlayer_activations.T.dot(d_output) *lr1\n",
    "    print(\"New Weights at Output Layer\\n\",wout1)\n",
    "\n",
    "    bout1 += np.sum(d_output, axis=0,keepdims=True) *lr1\n",
    "    print(\" New Bias at Output Layer \\n\",bout1)\n",
    "    \n",
    "    wh1 += X.T.dot(d_hiddenlayer) *lr1\n",
    "    print(\"New Weights at Hidden Layer\\n\",wh1)\n",
    "    bh1 += np.sum(d_hiddenlayer, axis=0,keepdims=True) *lr1\n",
    "    print(\" New Bias at Hidden Layer \\n\",bh1)\n",
    "\n",
    "print (\"\\n Final Output \\n\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
